{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siddsuresh97/prep_tutorial/blob/main/tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtRbaxMWSbF4"
      },
      "outputs": [],
      "source": [
        "Size\n",
        "# Download data\n",
        "# Generate activation of avg_pool and pool1\n",
        "# Perform experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "BASE_DIR = os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qNnGKH1LXwYi"
      },
      "outputs": [],
      "source": [
        "#@title imports\n",
        "\n",
        "import logging\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "import deepdish as dd\n",
        "\n",
        "from tensorflow.keras.backend import clear_session\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input as preprocess_input_resnet50\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title helper functions - intermediate layer features\n",
        "\n",
        "\n",
        "def store_dataset_fnames(intermediate_layer_names, dataset_dir, batch_size, stim_type, features_dir):\n",
        "    # Stores filenames of stimuli in a pickle file\n",
        "    # Fnames of stimuli contain metadata about labels\n",
        "    # Only one intermediate layer is used because the fnames are same regardless of the intermediate layer\n",
        "    datagen = ImageDataGenerator(preprocessing_function=preprocess_input_resnet50)\n",
        "    if not os.path.exists(features_dir):\n",
        "        os.makedirs(features_dir)\n",
        "    for layer in intermediate_layer_names[0]:\n",
        "        generator = datagen.flow_from_directory(dataset_dir, shuffle = False, batch_size = batch_size)\n",
        "        filenames = generator.filenames\n",
        "        fname_dict = {'fnames':filenames}\n",
        "        pickle.dump( fname_dict, open(os.path.join(features_dir,\"filenames_{}.p\".format(stim_type)), \"wb\" ))  \n",
        "    logging.info(datetime.now().strftime(\"%H:%M:%S\"))\n",
        "\n",
        "\n",
        "def store_intermediate_layer_features(model_name, intermediate_layer_names, dataset_dir, batch_size, stim_type, features_dir):\n",
        "    for layer in intermediate_layer_names:\n",
        "        logging.info('------------------------------- {} ----------------------------'.format(layer))\n",
        "        if model_name == 'resnet50':\n",
        "            datagen = ImageDataGenerator(preprocessing_function=preprocess_input_resnet50)\n",
        "        else:\n",
        "            logging.error('Models apart from resnet50 not implemented')\n",
        "        generator = datagen.flow_from_directory(dataset_dir, shuffle = False, batch_size = batch_size, target_size = (224, 224))\n",
        "        len = generator.n\n",
        "        batches = np.ceil(len/batch_size)\n",
        "        extract_and_store(model_name, 1, layer, generator, features_dir, stim_type, batches)\n",
        "        extract_and_store(model_name, 2, layer, generator, features_dir, stim_type, batches)\n",
        "    return\n",
        "\n",
        "\n",
        "def extract_intermediate_layer_representations(model_name, intermediate_layer_names, dataset_base_dir, batch_size, stim_type, features_base_dir, conditions, exp_name):\n",
        "    \"\"\"\n",
        "    This function extracts and stores intermediate layer representations given a model\n",
        "    and a dataset   \n",
        "    \"\"\"\n",
        "    if exp_name == '1a':\n",
        "        stim_name = 'random_stim'\n",
        "    elif exp_name == '1b':\n",
        "        stim_name = 'test_stim' \n",
        "    else:\n",
        "        logging.error('Only Exp 1a, 1b implemented')\n",
        "\n",
        "    # condition is set size if exp is 1a or 1b, otherwise it is color diveristy\n",
        "    for condition in conditions:\n",
        "        if exp_name in ['1a' or '1b']:\n",
        "            dataset_dir = os.path.join(dataset_base_dir, '{}_generated_stimuli'.format(condition), stim_name)\n",
        "            features_dir = os.path.join(features_base_dir, 'set_size_{}'.format(condition))\n",
        "        elif exp_name in ['2a']:\n",
        "            stim_type = condition\n",
        "            dataset_dir = os.path.join(dataset_base_dir, condition)\n",
        "            features_dir = os.path.join(features_base_dir)\n",
        "        else:\n",
        "            logging.error('Only 1a, 1b, 2a activation extraction implemented')\n",
        "        store_dataset_fnames(intermediate_layer_names, dataset_dir, batch_size, stim_type, features_dir)\n",
        "        start = time.time()\n",
        "        store_intermediate_layer_features(model_name, intermediate_layer_names, dataset_dir, batch_size, stim_type, features_dir)\n",
        "        logging.info('Total time to extract intermediate layer reprsentations (in seconds): {}'.format(time.time()-start))\n",
        "\n",
        "\n",
        "def extract_and_store(model_name, part, layer, generator, features_dir, stim_type, batches):\n",
        "    '''\n",
        "    Extracts intermediate layer features and stores them in two h5 files\n",
        "    '''\n",
        "    if model_name == 'resnet50':\n",
        "        model = ResNet50(weights='imagenet', include_top=True)\n",
        "    else:\n",
        "        logging.error('Models apart from resnet not implementde')\n",
        "    extractor = tf.keras.Model(inputs=model.inputs,\n",
        "                                outputs=[model.get_layer(layer).output])\n",
        "    features_dict = {'fnames':[],'features':[]}\n",
        "    if part == 1:\n",
        "        min_range = 0\n",
        "        max_range = int(batches)//2\n",
        "    elif part == 2:\n",
        "        min_range = int(batches)//2\n",
        "        max_range = int(batches)\n",
        "    for batch in range(min_range, max_range):\n",
        "        time_for_generator_operation = time.time()\n",
        "        x,y = generator.next()\n",
        "        # logging.info('Time for generator %f' % time.time()-time_for_generator_operation)\n",
        "        time_for_prediction = time.time()\n",
        "        generator_features = extractor.predict(x)\n",
        "        features_dict['features'].append(generator_features)\n",
        "        # logging.info('Prediction Time = %f'%time.time()-time_for_prediction)\n",
        "        time_for_deletion = time.time()\n",
        "        del generator_features\n",
        "        # logging.info('Time_for_deletion = ', time.time()-time_for_deletion)\n",
        "        idx = (generator.batch_index - 1) * generator.batch_size\n",
        "        features_dict['fnames'].append(generator.filenames[idx : idx + generator.batch_size])\n",
        "    del extractor\n",
        "    del model\n",
        "    if not os.path.exists(features_dir):\n",
        "        os.makedirs(features_dir)\n",
        "    dd.io.save(os.path.join(features_dir, 'resnet_50_features_{}_{}_part_{}.h5'.format(stim_type, layer, part)), features_dict)\n",
        "    del features_dict\n",
        "    gc.collect()\n",
        "    clear_session()\n",
        "    logging.info(\"Saved {} part {}\".format(layer, part))\n",
        "    return \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xtIfpUqzYy9R"
      },
      "outputs": [],
      "source": [
        "#@title link to drive "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Average Size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract intermediate layer representations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Metal device set to: Apple M1 Pro\n",
            "\n",
            "systemMemory: 16.00 GB\n",
            "maxCacheSize: 5.33 GB\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-07-06 10:04:19.526108: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2022-07-06 10:04:19.526436: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
            "2022-07-06 10:04:23.645300: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
            "2022-07-06 10:04:24.022264: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8/8 [==============================] - 3s 126ms/step\n",
            "> \u001b[0;32m/var/folders/qt/rt9bv7zs62b05gjym3zzqgyw0000gn/T/ipykernel_59931/4258867819.py\u001b[0m(93)\u001b[0;36mextract_and_store\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     92 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 93 \u001b[0;31m        \u001b[0mfeatures_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fnames'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     94 \u001b[0;31m    \u001b[0;32mdel\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "model_name = 'resnet50'\n",
        "intermediate_layer_names = ['avg_pool', 'pool1_pool']\n",
        "dataset_dir = os.path.join(BASE_DIR, \"data/average_size\")\n",
        "batch_size = 256\n",
        "stim_type = 'avg_size'\n",
        "features_dir = os.path.join(dataset_dir, \"features\")\n",
        "conditions = [4, 8]\n",
        "exp_name = '1a'\n",
        "extract_intermediate_layer_representations(model_name=model_name,\n",
        "                                            intermediate_layer_names = intermediate_layer_names,\n",
        "                                            dataset_base_dir = dataset_dir, \n",
        "                                            batch_size = batch_size, \n",
        "                                            stim_type = stim_type, \n",
        "                                            features_base_dir = features_dir, \n",
        "                                            conditions = conditions, \n",
        "                                            exp_name = exp_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "tutorial.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('ensemble_representations')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "d51e2fff98dacac56a168894eab35c66d31737be860e3214143049a454647912"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
