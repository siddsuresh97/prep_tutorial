{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siddsuresh97/prep_tutorial/blob/main/tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "0yPo3BgzQEit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download files - run this only once\n",
        "\n",
        "import zipfile\n",
        "!wget --no-check-certificate \\\n",
        "    \"https://github.com/siddsuresh97/prep_tutorial/archive/refs/heads/main.zip\" \\\n",
        "    -O \"/tmp/prep_cnn_psychophysics.zip\"\n",
        "\n",
        "\n",
        "zip_ref = zipfile.ZipFile('/tmp/prep_cnn_psychophysics.zip', 'r') #Opens the zip file in read mode\n",
        "zip_ref.extractall('/tmp') #Extracts the files into the /tmp folder\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "_eY8yM9wIKa6",
        "outputId": "4115ee03-4df2-4176-f41f-42491f9f6e3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-06 15:33:03--  https://github.com/siddsuresh97/prep_tutorial/archive/refs/heads/main.zip\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/siddsuresh97/prep_tutorial/zip/refs/heads/main [following]\n",
            "--2022-07-06 15:33:03--  https://codeload.github.com/siddsuresh97/prep_tutorial/zip/refs/heads/main\n",
            "Resolving codeload.github.com (codeload.github.com)... 140.82.112.10\n",
            "Connecting to codeload.github.com (codeload.github.com)|140.82.112.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘/tmp/prep_cnn_psychophysics.zip’\n",
            "\n",
            "/tmp/prep_cnn_psych     [       <=>          ] 100.93M  19.5MB/s    in 5.3s    \n",
            "\n",
            "2022-07-06 15:33:08 (19.1 MB/s) - ‘/tmp/prep_cnn_psychophysics.zip’ saved [105834558]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## If restart runtime"
      ],
      "metadata": {
        "id": "SCWmYvfTQHYE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qNnGKH1LXwYi",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title imports\n",
        "\n",
        "!pip install deepdish -q\n",
        "\n",
        "import logging\n",
        "import random\n",
        "import ast\n",
        "import zipfile\n",
        "import gc\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "import deepdish as dd\n",
        "\n",
        "from tensorflow.keras.backend import clear_session\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input as preprocess_input_resnet50\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import MultiLabelBinarizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JFOIVV0XIEqZ"
      },
      "outputs": [],
      "source": [
        "BASE_DIR = '/tmp/prep_tutorial-main'\n",
        "RESULTS_DIR = os.path.join(BASE_DIR, \"results/\")\n",
        "BATCH_SIZE = 512\n",
        "INTERMEDIATE_LAYER_NAMES = ['avg_pool', 'pool1_pool']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "sJfxRVikIEqZ"
      },
      "outputs": [],
      "source": [
        "#@title helper functions - intermediate layer features\n",
        "\n",
        "\n",
        "def store_dataset_fnames(intermediate_layer_names, dataset_dir, batch_size, stim_type, features_dir):\n",
        "    # Stores filenames of stimuli in a pickle file\n",
        "    # Fnames of stimuli contain metadata about labels\n",
        "    # Only one intermediate layer is used because the fnames are same regardless of the intermediate layer\n",
        "    datagen = ImageDataGenerator(preprocessing_function=preprocess_input_resnet50)\n",
        "    if not os.path.exists(features_dir):\n",
        "        os.makedirs(features_dir)\n",
        "    for layer in intermediate_layer_names[0]:\n",
        "        generator = datagen.flow_from_directory(dataset_dir, shuffle = False, batch_size = batch_size)\n",
        "        filenames = generator.filenames\n",
        "        fname_dict = {'fnames':filenames}\n",
        "        pickle.dump( fname_dict, open(os.path.join(features_dir,\"filenames_{}.p\".format(stim_type)), \"wb\" ))  \n",
        "    logging.info(datetime.now().strftime(\"%H:%M:%S\"))\n",
        "\n",
        "\n",
        "def store_intermediate_layer_features(model_name, intermediate_layer_names, dataset_dir, batch_size, stim_type, features_dir):\n",
        "    for layer in intermediate_layer_names:\n",
        "        logging.info('------------------------------- {} ----------------------------'.format(layer))\n",
        "        if model_name == 'resnet50':\n",
        "            datagen = ImageDataGenerator(preprocessing_function=preprocess_input_resnet50)\n",
        "        else:\n",
        "            logging.error('Models apart from resnet50 not implemented')\n",
        "        generator = datagen.flow_from_directory(dataset_dir, shuffle = False, batch_size = batch_size, target_size = (224, 224))\n",
        "        len = generator.n\n",
        "        batches = np.ceil(len/batch_size)\n",
        "        extract_and_store(model_name, 1, layer, generator, features_dir, stim_type, batches)\n",
        "        extract_and_store(model_name, 2, layer, generator, features_dir, stim_type, batches)\n",
        "    return\n",
        "\n",
        "\n",
        "def extract_intermediate_layer_representations(model_name, intermediate_layer_names, dataset_base_dir, batch_size, stim_type, features_base_dir, conditions, exp_name):\n",
        "    \"\"\"\n",
        "    This function extracts and stores intermediate layer representations given a model\n",
        "    and a dataset   \n",
        "    \"\"\"\n",
        "    if exp_name == '1a':\n",
        "        stim_name = 'random_stim'\n",
        "    elif exp_name == '1b':\n",
        "        stim_name = 'test_stim' \n",
        "    else:\n",
        "        logging.error('Only Exp 1a, 1b implemented')\n",
        "\n",
        "    # condition is set size if exp is 1a or 1b, otherwise it is color diveristy\n",
        "    for condition in conditions:\n",
        "        if exp_name in ['1a', '1b']:\n",
        "            dataset_dir = os.path.join(dataset_base_dir, '{}_generated_stimuli'.format(condition), stim_name)\n",
        "            features_dir = os.path.join(features_base_dir, 'set_size_{}'.format(condition))\n",
        "        elif exp_name in ['2a']:\n",
        "            stim_type = condition\n",
        "            dataset_dir = os.path.join(dataset_base_dir, condition)\n",
        "            features_dir = os.path.join(features_base_dir)\n",
        "        else:\n",
        "            logging.error('Only 1a, 1b, 2a activation extraction implemented')\n",
        "        store_dataset_fnames(intermediate_layer_names, dataset_dir, batch_size, stim_type, features_dir)\n",
        "        start = time.time()\n",
        "        store_intermediate_layer_features(model_name, intermediate_layer_names, dataset_dir, batch_size, stim_type, features_dir)\n",
        "        logging.info('Total time to extract intermediate layer reprsentations (in seconds): {}'.format(time.time()-start))\n",
        "\n",
        "\n",
        "def extract_and_store(model_name, part, layer, generator, features_dir, stim_type, batches):\n",
        "    '''\n",
        "    Extracts intermediate layer features and stores them in two h5 files\n",
        "    '''\n",
        "    if model_name == 'resnet50':\n",
        "        model = ResNet50(weights='imagenet', include_top=True)\n",
        "    else:\n",
        "        logging.error('Models apart from resnet not implementde')\n",
        "    extractor = tf.keras.Model(inputs=model.inputs,\n",
        "                                outputs=[model.get_layer(layer).output])\n",
        "    features_dict = {'fnames':[],'features':[]}\n",
        "    if part == 1:\n",
        "        min_range = 0\n",
        "        max_range = int(batches)//2\n",
        "    elif part == 2:\n",
        "        min_range = int(batches)//2\n",
        "        max_range = int(batches)\n",
        "    for batch in range(min_range, max_range):\n",
        "        time_for_generator_operation = time.time()\n",
        "        x,y = generator.next()\n",
        "        # logging.info('Time for generator %f' % time.time()-time_for_generator_operation)\n",
        "        time_for_prediction = time.time()\n",
        "        generator_features = extractor.predict(x)\n",
        "        features_dict['features'].append(generator_features)\n",
        "        # logging.info('Prediction Time = %f'%time.time()-time_for_prediction)\n",
        "        time_for_deletion = time.time()\n",
        "        del generator_features\n",
        "        # logging.info('Time_for_deletion = ', time.time()-time_for_deletion)\n",
        "        idx = (generator.batch_index - 1) * generator.batch_size\n",
        "        features_dict['fnames'].append(generator.filenames[idx : idx + generator.batch_size])\n",
        "    del extractor\n",
        "    del model\n",
        "    if not os.path.exists(features_dir):\n",
        "        os.makedirs(features_dir)\n",
        "    dd.io.save(os.path.join(features_dir, 'resnet_50_features_{}_{}_part_{}.h5'.format(stim_type, layer, part)), features_dict)\n",
        "    del features_dict\n",
        "    gc.collect()\n",
        "    clear_session()\n",
        "    logging.info(\"Saved {} part {}\".format(layer, part))\n",
        "    return \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title helper functions - Run experiment\n",
        "\n",
        "def run_exp(exp_name, features_dir, subsampling_levels, conditions, \n",
        "            intermediate_layer_names, results_dir, result_fname, performance_measure):\n",
        "    logging.info(\"Running experiement %s\" % exp_name)\n",
        "    if exp_name == '1c':\n",
        "        stim_type = 'test_stim'\n",
        "        analysis_fn = logistic_regression\n",
        "        analysis_type = 'logistic_regression'\n",
        "    elif exp_name in ['1a', '1b']:\n",
        "        if exp_name == '1a':\n",
        "            stim_type = 'ind'\n",
        "        elif exp_name == '1b':\n",
        "            stim_type = 'test_stim'\n",
        "        analysis_fn = linear_regression\n",
        "        analysis_type = 'linear_regression'\n",
        "    else:\n",
        "        logging.debug(\"NOT IMPLEMENTED FOR OTHER EXPERIMENTS\")\n",
        "    if \"1\" in exp_name:\n",
        "        set_sizes = conditions\n",
        "        for set_size in set_sizes:\n",
        "            logging.info('########  SET SIZE %d #########'%set_size)\n",
        "            intermediate_layer_feats_dir = os.path.join(features_dir, 'set_size_{}/'.format(set_size))\n",
        "            start_time = time.time()\n",
        "            number_of_random_features = subsampling_levels\n",
        "            results_test = {}\n",
        "            # results_number_of_samples = {}\n",
        "            for layer in intermediate_layer_names:\n",
        "                logging.info('####################       %s          ##########################'%layer)\n",
        "                network_results_test = {}\n",
        "                # network_results_number_of_samples = {}\n",
        "                # test_stim contains circles of multiple set sizes and ind stim only contains a single circle\n",
        "                data, labels = get_data_and_labels(stim_type = stim_type,\n",
        "                                                layer = layer,\n",
        "                                                intermediate_layer_feats_dir = intermediate_layer_feats_dir,\n",
        "                                                analysis_type = analysis_type, \n",
        "                                                condition = set_size)\n",
        "                for random_features_number in number_of_random_features:\n",
        "                    network_results_test[random_features_number] = {}\n",
        "                    # network_results_number_of_samples[random_features_number] = {}\n",
        "                    if data.shape[1]<random_features_number:\n",
        "                        network_results_test.update({random_features_number:float('nan')})\n",
        "                        continue\n",
        "                    # TO DO :- only sparse distribution now, maybe implement dim reduction too\n",
        "                    subsample_data = subsample_img_features(data, random_features_number, 100)\n",
        "                    if analysis_type == 'logistic_regression':\n",
        "                        for circle_size in range(labels.shape[1]):\n",
        "                            evalutaion_measure = analysis_fn(subsample_data, labels[:,circle_size], performance_measure, exp_name)\n",
        "                            network_results_test[random_features_number].update({circle_size:evalutaion_measure})\n",
        "                            # network_results_number_of_samples[random_features_number].update({circle_size:samples})\n",
        "                    elif analysis_type == 'linear_regression':\n",
        "                        evalutaion_measure = analysis_fn(subsample_data, labels, performance_measure, exp_name)\n",
        "                        network_results_test[random_features_number] = evalutaion_measure\n",
        "                    else:\n",
        "                        logging.error('Analysis type not implemented')\n",
        "                results_test.update({layer:network_results_test})\n",
        "                # results_number_of_samples.update({layer:network_results_number_of_samples})\n",
        "            dir_path = os.path.join(results_dir, exp_name, 'set_size_{}'.format(set_size))\n",
        "            if not os.path.exists(dir_path):\n",
        "                os.makedirs(dir_path)\n",
        "            with open(os.path.join(results_dir,exp_name, 'set_size_{}'.format(set_size), result_fname), 'wb') as f:\n",
        "                pickle.dump(results_test,f)\n",
        "            # with open(os.path.join(results_dir,exp_name, 'set_size_{}'.format(set_size), result_fname), 'wb') as f:\n",
        "            #   pickle.dump(results_number_of_samples,f)\n",
        "            logging.info('Total time %d'%(time.time()-start_time))\n",
        "    elif \"2\" in exp_name:\n",
        "        if exp_name == \"2a\":\n",
        "            analysis_type = 'logistic_regression'\n",
        "            analysis_fn = logistic_regression\n",
        "        elif exp_name == \"2b\":\n",
        "            analysis_type = 'logistic_regression'\n",
        "            analysis_fn = logistic_regression \n",
        "        elif exp_name == \"2c\":\n",
        "            analysis_type = 'logistic_regression'\n",
        "            analysis_fn = logistic_regression \n",
        "        else:\n",
        "            logging.error('Other experiments not implemented')\n",
        "        results_test = {}\n",
        "        number_of_random_features = subsampling_levels\n",
        "        start_time = time.time()\n",
        "        for layer in intermediate_layer_names:\n",
        "            network_results_test = {}\n",
        "            logging.info('####################       {}          ##########################'.format(layer))\n",
        "            if exp_name == \"2a\":\n",
        "                low_div_data, low_div_labels = get_data_and_labels(stim_type = 'low_diversity', layer = layer, \n",
        "                                                                intermediate_layer_feats_dir = features_dir, \n",
        "                                                                analysis_type='logistic_regression', \n",
        "                                                                condition='low_diversity')\n",
        "                high_div_data, high_div_labels = get_data_and_labels(stim_type = 'high_diversity', layer = layer, \n",
        "                                                                intermediate_layer_feats_dir = features_dir, \n",
        "                                                                analysis_type='logistic_regression', \n",
        "                                                                condition='high_diversity')\n",
        "            elif exp_name == \"2b\":\n",
        "                low_div_data, low_div_labels = get_data_and_labels(stim_type = 'low_diversity', layer = layer, \n",
        "                                                                intermediate_layer_feats_dir = features_dir, \n",
        "                                                                analysis_type='logistic_regression', \n",
        "                                                                condition='ind_color')\n",
        "                high_div_data, high_div_labels = get_data_and_labels(stim_type = 'high_diversity', layer = layer, \n",
        "                                                                intermediate_layer_feats_dir = features_dir, \n",
        "                                                                analysis_type='logistic_regression', \n",
        "                                                                condition='ind_color')\n",
        "            elif exp_name == \"2c\":\n",
        "                low_div_data, low_div_labels = get_data_and_labels(stim_type = 'low_diversity', layer = layer, \n",
        "                                                                intermediate_layer_feats_dir = features_dir, \n",
        "                                                                analysis_type='logistic_regression', \n",
        "                                                                condition='ind_letter')\n",
        "                high_div_data, high_div_labels = get_data_and_labels(stim_type = 'high_diversity', layer = layer, \n",
        "                                                                intermediate_layer_feats_dir = features_dir, \n",
        "                                                                analysis_type='logistic_regression', \n",
        "                                                                condition='ind_letter')\n",
        "            else:\n",
        "                logging.error('only implemented 2abc in exp 2')                    \n",
        "            data = np.concatenate((low_div_data, high_div_data))\n",
        "            labels = np.concatenate((low_div_labels, high_div_labels))\n",
        "            for random_features_number in number_of_random_features:\n",
        "                network_results_test[random_features_number] = {}\n",
        "                if data.shape[1]<random_features_number:\n",
        "                    network_results_test.update({random_features_number:float('nan')})\n",
        "                    continue\n",
        "                subsample_data = subsample_img_features(data, random_features_number, 100)\n",
        "                if exp_name == '2a':\n",
        "                    evalutaion_measure = analysis_fn(subsample_data, labels, performance_measure, exp_name)\n",
        "                    network_results_test.update({random_features_number:evalutaion_measure})\n",
        "                elif exp_name == '2b':\n",
        "                    for color in range(labels.shape[1]):\n",
        "                        evalutaion_measure = analysis_fn(subsample_data, labels[:,color], performance_measure, exp_name)\n",
        "                        network_results_test[random_features_number].update({color:evalutaion_measure})\n",
        "                elif exp_name == '2c':\n",
        "                    temp = []\n",
        "                    for letter in range(labels.shape[1]):\n",
        "                        evalutaion_measure = analysis_fn(subsample_data, labels[:,letter], performance_measure, exp_name)\n",
        "                        temp.append(evalutaion_measure)\n",
        "                        network_results_test[random_features_number].update({letter:evalutaion_measure})\n",
        "                else:\n",
        "                    logging.error('Only logistic regression implemented for experiment 2a and 2b')\n",
        "            results_test.update({layer:network_results_test})\n",
        "            dir_path = os.path.join(results_dir, exp_name)\n",
        "            if not os.path.exists(dir_path):\n",
        "                os.makedirs(dir_path)\n",
        "            with open(os.path.join(results_dir,exp_name, result_fname), 'wb') as f:\n",
        "                pickle.dump(results_test, f)\n",
        "            logging.info('Total time %d'%(time.time()-start_time))\n",
        "\n",
        "\n",
        "def logistic_regression(data, labels, performance_measure, exp_name):\n",
        "    if exp_name == '1c':\n",
        "        label_0_idx = np.where(labels==0)[0]\n",
        "        label_1_idx = np.where(labels==1)[0]\n",
        "        num_stimuli = 430\n",
        "        label_0_data = data[label_0_idx[:num_stimuli]]\n",
        "        label_1_data = data[label_1_idx[:num_stimuli]]\n",
        "        data = np.concatenate((label_0_data, label_1_data))\n",
        "        labels = np.concatenate((np.zeros(num_stimuli), np.ones(num_stimuli))).reshape(-1)\n",
        "        logging.info('data shape : %r, labels shape : %r, label_0 : %r, label:1,  %r'%(data.shape, labels.shape, label_0_idx.shape, label_1_idx.shape))\n",
        "    \n",
        "    elif exp_name == '2a':\n",
        "        # this is because we want to use all the data and labels for exp 2a\n",
        "        pass\n",
        "    elif exp_name in ['2b', '2c']:\n",
        "        label_0_idx = np.where(labels==0)[0]\n",
        "        label_1_idx = np.where(labels==1)[0]\n",
        "        if exp_name == '2b':\n",
        "            num_stimuli = 329\n",
        "        elif exp_name == '2c':\n",
        "           num_stimuli = 773\n",
        "        label_0_data = data[label_0_idx[:num_stimuli]]\n",
        "        label_1_data = data[label_1_idx[:num_stimuli]]\n",
        "        data = np.concatenate((label_0_data, label_1_data))\n",
        "        labels = np.concatenate((np.zeros(num_stimuli), np.ones(num_stimuli))).reshape(-1)\n",
        "        logging.info('data shape : %r, labels shape : %r, label_0 : %r, label:1,  %r'%(data.shape, labels.shape, label_0_idx.shape, label_1_idx.shape))\n",
        "    # x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.1, random_state=42)\n",
        "    # baseline_acc_test = max(np.count_nonzero(y_test==0)/y_test.shape[0], np.count_nonzero(y_test==1)/y_test.shape[0])\n",
        "    cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
        "    model = LogisticRegression(random_state=1, max_iter=1000, solver='liblinear')\n",
        "    scores = cross_val_score(model, data, labels, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "    DEBUG = True\n",
        "    if DEBUG:\n",
        "        y_pred = cross_val_predict(model, data, labels, cv=10)\n",
        "        conf_mat = confusion_matrix(labels, y_pred)\n",
        "        logging.info('Confusion matrix : %r'%conf_mat)\n",
        "    # model.fit(x_train, y_train)\n",
        "    logging.info('test data label 0 :%r,  label 1:%r'%(np.where(data==0)[0].shape, np.where(labels==1)[0].shape))\n",
        "    if performance_measure == 'accuracy':\n",
        "        # return model.score(x_test, y_test)\n",
        "        return np.mean(scores)\n",
        "    else:\n",
        "        logging.error(\"Performance measure not implemented\")\n",
        "    # return num_stimuli\n",
        "\n",
        "def linear_regression(data, labels, performance_measure, exp_name):\n",
        "    # x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.25, random_state=42)\n",
        "    kf = KFold(n_splits=10, random_state=1, shuffle=True)\n",
        "    percentage_abs_error_list = []\n",
        "    rmspe_list = []\n",
        "    rmse_list = []\n",
        "    for train_index, test_index in kf.split(data):\n",
        "        x_train, x_test = data[train_index], data[test_index]\n",
        "        y_train, y_test = labels[train_index], labels[test_index]\n",
        "        model = LinearRegression() \n",
        "        model.fit(x_train,y_train)\n",
        "        y_pred = model.predict(x_test)\n",
        "        percentage_abs_error = np.average(np.abs(y_pred-y_test)/y_test) * 100\n",
        "        percentage_abs_error_list.append(percentage_abs_error) \n",
        "        rmspe = (np.sqrt(np.mean(np.square((y_test - y_pred) / y_test)))) * 100\n",
        "        rmspe_list.append(rmspe)\n",
        "        rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
        "        rmse_list.append(rmse)\n",
        "    # rmse_train = np.sqrt(metrics.mean_squared_error(y_train, model.predict(x_train)))\n",
        "    if performance_measure == 'percentage_abs_error':\n",
        "        return np.mean(percentage_abs_error_list)\n",
        "    elif performance_measure == 'rmspe':\n",
        "        return np.mean(rmspe_list)\n",
        "    elif performance_measure == 'rmse':\n",
        "        return np.mean(rmse_list)\n",
        "    else:\n",
        "        logging.error(\"Performance measure not implemented\")\n",
        "\n",
        "\n",
        "\n",
        "def get_data_and_labels(stim_type, layer, intermediate_layer_feats_dir, analysis_type, condition):\n",
        "    \"\"\"\n",
        "    Returns intermediate layer features and corresponding\n",
        "    labels. \n",
        "    Labels are one hot encoded for logistic regression and \n",
        "    float (average size otherwise)\n",
        "\n",
        "    condition is set size for avg.size experiments and color diversity otherwise\n",
        "    \"\"\"\n",
        "    part_1_fname = 'resnet_50_features_{}_{}_part_1.h5'.format(stim_type, layer)\n",
        "    part_2_fname = 'resnet_50_features_{}_{}_part_2.h5'.format(stim_type, layer)\n",
        "    try:\n",
        "        data_1 = np.concatenate(np.asarray(dd.io.load(os.path.join(intermediate_layer_feats_dir, part_1_fname))['features'],dtype = 'float16'))\n",
        "        if layer != 'avg_pool':\n",
        "            data_1 = np.squeeze(np.apply_over_axes(np.mean, data_1, [1, 2]))\n",
        "    except ValueError:\n",
        "        # occurs because part_1 file is empty because batch_size < dataset_size\n",
        "        logging.info('Value error while loading data 1')\n",
        "        logging.info(\"%r %r %r %r %r\", stim_type, layer, intermediate_layer_feats_dir, analysis_type, condition)\n",
        "    logging.info('dict 1 loaded')\n",
        "    data_2 = np.concatenate(np.asarray(dd.io.load(os.path.join(intermediate_layer_feats_dir, part_2_fname))['features']))\n",
        "    if layer != 'avg_pool':\n",
        "        data_2 = np.squeeze(np.apply_over_axes(np.mean, data_2, [1, 2]))\n",
        "    if stim_type in ['low_diversity', 'high_diversity']: # because of value error, data_1 is empty\n",
        "        data = data_2\n",
        "    else:\n",
        "        data = np.concatenate((data_1, data_2))\n",
        "    data.astype('float16')\n",
        "    logging.info('dict 2 loaded')\n",
        "    fnames = pickle.load(open(os.path.join(intermediate_layer_feats_dir , \"filenames_{}.p\".format(stim_type)), \"rb\" ))\n",
        "    labels = []\n",
        "    if analysis_type == 'logistic_regression':\n",
        "        if condition in [4, 8, 10, 16]:\n",
        "            for fname in fnames['fnames']:\n",
        "                temp = fname.split('_')[-1].split('.png')\n",
        "                radii = ast.literal_eval(temp[0])\n",
        "                # pre processing code for logistic regression dataset with no confounds\n",
        "                # temp = fname.split('radii')[-1].split('.png')[0]\n",
        "                # temp = temp[1:-1]\n",
        "                # temp = temp.split(' ')\n",
        "                # temp = [i[:-1] if '_' in i else i for i in temp]\n",
        "                # radii = [float(i) for i in temp if i != '']\n",
        "                assert(len(radii) == condition)\n",
        "                labels.append(radii)\n",
        "            one_hot = MultiLabelBinarizer()\n",
        "            one_hot_encoded = one_hot.fit_transform(labels)\n",
        "            assert(np.asarray(data).shape[0]==np.asarray(labels).shape[0])\n",
        "            return np.asarray(data), np.asarray(one_hot_encoded)\n",
        "        elif condition in ['low_diversity', 'high_diversity']:\n",
        "            for fname in fnames['fnames']:\n",
        "                if stim_type == 'low_diversity':\n",
        "                    labels.append(0)\n",
        "                elif stim_type == 'high_diversity':\n",
        "                    labels.append(1)\n",
        "                else:\n",
        "                    logging.error('Only high and low diversity stimuli allowed')\n",
        "            return np.asarray(data), np.asarray(labels)\n",
        "        elif condition in ['ind_color']:\n",
        "            for fname in fnames['fnames']:\n",
        "                temp = fname.split('test_array_')[-1].split('_[')\n",
        "                colors_rgb = ast.literal_eval(temp[0])\n",
        "                labels.append(colors_rgb)\n",
        "            assert(np.asarray(data).shape[0]==np.asarray(labels).shape[0])\n",
        "            one_hot = MultiLabelBinarizer()\n",
        "            one_hot_encoded_colors = one_hot.fit_transform(labels)\n",
        "            return np.asarray(data), np.asarray(one_hot_encoded_colors)\n",
        "        elif condition in ['ind_letter']:\n",
        "            for fname in fnames['fnames']:\n",
        "                letters = [i for i in fname.split(']_[')[-1].split('.png')[0] if i.isupper()]\n",
        "                labels.append(letters)\n",
        "            assert(np.asarray(data).shape[0]==np.asarray(labels).shape[0])\n",
        "            one_hot = MultiLabelBinarizer()\n",
        "            one_hot_encoded_letters = one_hot.fit_transform(labels)\n",
        "            return np.asarray(data), np.asarray(one_hot_encoded_letters)\n",
        "        else:\n",
        "            logging.error('Only color diversity and average size experiments implemented')\n",
        "\n",
        "    elif analysis_type == 'linear_regression':\n",
        "        if condition in [4, 8, 10, 16]:\n",
        "            if stim_type in ['test_stim', 'ind']:\n",
        "                for fname in fnames['fnames']:\n",
        "                    temp = fname.split('_')[-1].split('.png')\n",
        "                    radii = ast.literal_eval(temp[0])\n",
        "                    # pre processing code for logistic regression dataset with no confounds\n",
        "                    # temp = fname.split('radii')[-1].split('.png')[0]\n",
        "                    # temp = temp[1:-1]\n",
        "                    # temp = temp.split(' ')\n",
        "                    # temp = [i[:-1] if '_' in i else i for i in temp]\n",
        "                    # radii = [float(i) for i in temp if i != '']\n",
        "                    if stim_type == 'ind':\n",
        "                        # Sanity check stimuli have single circles so there should be only 1 radius in the list\n",
        "                        # assert(type(radii[0])== float)\n",
        "                        assert(type(radii)== float)\n",
        "                        # assert(len(radii) == 1)\n",
        "                    else:\n",
        "                        assert(len(radii) == condition)\n",
        "                    labels.append(np.mean(radii))\n",
        "                assert(np.asarray(data).shape[0]==np.asarray(labels).shape[0])\n",
        "                return np.asarray(data), np.asarray(labels)\n",
        "            else:\n",
        "                logging.error(\"Only Ind and Test Stim implemented\")\n",
        "        else:\n",
        "            print('Only average size implemented for linear regression')\n",
        "    else:\n",
        "        logging.error(\"Invalid type of analysis. Only linear regression and logistic regression implemented\")\n",
        "\n",
        "    \n",
        "\n",
        "def subsample_img_features(data, n_features, seed):\n",
        "    \"\"\"\n",
        "    Subsamples data(img_features) \n",
        "    TO DO - implement tsne\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    indices = random.sample(range(0, data.shape[1]), n_features)\n",
        "    data_subset = []\n",
        "    for instance in data:\n",
        "        data_subset.append([instance[index] for index in indices])\n",
        "    return np.asarray(data_subset)\n",
        "        "
      ],
      "metadata": {
        "cellView": "form",
        "id": "y_MTDuBoT-uZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncezp9kmIEqb"
      },
      "source": [
        "# Average Size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtRbaxMWSbF4"
      },
      "outputs": [],
      "source": [
        "Size\n",
        "# Download data\n",
        "# Generate activation of avg_pool and pool1\n",
        "# Perform experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Efi-ghuIEqb"
      },
      "source": [
        "## Extract intermediate layer representations - should take about 10 min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GMvdZxAxIEqc",
        "outputId": "a34b3b19-571e-4f3d-ad75-82d467940738",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n"
          ]
        }
      ],
      "source": [
        "model_name = 'resnet50'\n",
        "intermediate_layer_names = INTERMEDIATE_LAYER_NAMES\n",
        "dataset_dir = os.path.join(BASE_DIR, \"data/average_size\")\n",
        "batch_size = BATCH_SIZE\n",
        "stim_type = 'ind'\n",
        "features_dir = os.path.join(dataset_dir, \"features\")\n",
        "conditions = [4, 8]\n",
        "exp_name = '1a'\n",
        "extract_intermediate_layer_representations(model_name=model_name,\n",
        "                                            intermediate_layer_names = intermediate_layer_names,\n",
        "                                            dataset_base_dir = dataset_dir, \n",
        "                                            batch_size = batch_size, \n",
        "                                            stim_type = stim_type, \n",
        "                                            features_base_dir = features_dir, \n",
        "                                            conditions = conditions, \n",
        "                                            exp_name = exp_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "z69DouFGIEqc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb6c39d6-ec2f-4f54-bff2-00b714808b98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n",
            "Found 3000 images belonging to 1 classes.\n"
          ]
        }
      ],
      "source": [
        "model_name = 'resnet50'\n",
        "intermediate_layer_names = INTERMEDIATE_LAYER_NAMES\n",
        "dataset_dir = os.path.join(BASE_DIR, \"data/average_size\")\n",
        "batch_size = BATCH_SIZE\n",
        "stim_type = 'test_stim'\n",
        "features_dir = os.path.join(dataset_dir, \"features\")\n",
        "conditions = [4, 8]\n",
        "exp_name = '1b'\n",
        "extract_intermediate_layer_representations(model_name=model_name,\n",
        "                                            intermediate_layer_names = intermediate_layer_names,\n",
        "                                            dataset_base_dir = dataset_dir, \n",
        "                                            batch_size = batch_size, \n",
        "                                            stim_type = stim_type, \n",
        "                                            features_base_dir = features_dir, \n",
        "                                            conditions = conditions, \n",
        "                                            exp_name = exp_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiments"
      ],
      "metadata": {
        "id": "QxCBQ9nhR0fn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Are individual circle sizes represented by CNNs trained on naturalistic images?"
      ],
      "metadata": {
        "id": "LaBY0hrzR58D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exp_name = '1a'\n",
        "features_dir = os.path.join(dataset_dir, \"features\")\n",
        "subsampling_levels = [10, 20, 50]\n",
        "conditions = [4, 8]\n",
        "intermediate_layer_names = INTERMEDIATE_LAYER_NAMES\n",
        "results_dir = RESULTS_DIR\n",
        "performance_measure = 'rmspe'\n",
        "result_fname = 'sanity_check_rmspe'\n",
        "\n",
        "run_exp(exp_name = exp_name, \n",
        "            features_dir = features_dir, \n",
        "            subsampling_levels = subsampling_levels, \n",
        "            conditions = conditions, \n",
        "            intermediate_layer_names = intermediate_layer_names, \n",
        "            results_dir = RESULTS_DIR, \n",
        "            result_fname = result_fname, \n",
        "            performance_measure= performance_measure)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJLTAvM1R3-8",
        "outputId": "f7f0e6b1-047f-4c10-b001-77a3ce10812e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:240: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:240: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:240: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:240: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0AaXqdS2UUo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Is the ensemble representation of average size represented by CNNs trained on naturalistic images?"
      ],
      "metadata": {
        "id": "FwwSqvKPWei6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exp_name = '1b'\n",
        "features_dir = os.path.join(dataset_dir, \"features\")\n",
        "subsampling_levels = [10, 20, 50]\n",
        "conditions = [4, 8]\n",
        "intermediate_layer_names = INTERMEDIATE_LAYER_NAMES\n",
        "results_dir = RESULTS_DIR\n",
        "performance_measure = 'rmspe'\n",
        "result_fname = 'sanity_check_rmspe'\n",
        "\n",
        "run_exp(exp_name = exp_name, \n",
        "            features_dir = features_dir, \n",
        "            subsampling_levels = subsampling_levels, \n",
        "            conditions = conditions, \n",
        "            intermediate_layer_names = intermediate_layer_names, \n",
        "            results_dir = RESULTS_DIR, \n",
        "            result_fname = result_fname, \n",
        "            performance_measure= performance_measure)"
      ],
      "metadata": {
        "outputId": "6398f6b6-c687-4581-db9e-0a4217bd8b85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxAsiCj9Wei7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:240: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:240: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:240: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:240: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What happens to the individual representations in the ensemble?"
      ],
      "metadata": {
        "id": "BzUhF8i_78QA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exp_name = '1c'\n",
        "features_dir = os.path.join(dataset_dir, \"features\")\n",
        "subsampling_levels = [10, 20, 50]\n",
        "conditions = [4, 8]\n",
        "intermediate_layer_names = INTERMEDIATE_LAYER_NAMES\n",
        "results_dir = RESULTS_DIR\n",
        "performance_measure = 'rmspe'\n",
        "result_fname = 'sanity_check_rmspe'\n",
        "\n",
        "run_exp(exp_name = exp_name, \n",
        "            features_dir = features_dir, \n",
        "            subsampling_levels = subsampling_levels, \n",
        "            conditions = conditions, \n",
        "            intermediate_layer_names = intermediate_layer_names, \n",
        "            results_dir = RESULTS_DIR, \n",
        "            result_fname = result_fname, \n",
        "            performance_measure= performance_measure)"
      ],
      "metadata": {
        "id": "3Ne5rzXr8FxE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "tutorial.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('ensemble_representations')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "d51e2fff98dacac56a168894eab35c66d31737be860e3214143049a454647912"
      }
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}